#Â -*-Â coding:Â utf-8Â -*-
import os
import re
import json
import jsonlines
# import thulac
# thuseg = thulac.thulac(seg_only=True, filt = False)
# import MeCab                                           #æ—¥è¯­åˆ†è¯
# mecab_tagger = MeCab.Tagger("-Owakati")


filedir = os.getcwd()+'/retrived_data_merged'
filenames = os.listdir(filedir)
file_result = os.getcwd() + "/pictures_countries_companies/for_draw_word_vector/"
# relevant_countries = ['us', 'uk', 'ca', 'au', 'nz', 'in', 'pk']
relevant_countries = ['in']
# relevant_companies = ['h', 't', 'b']
relevant_companies = ['b']
# badwords = ['çº¦ç‚®', 'çº¦ ç‚®', 'çˆ†ä¹³', 'æƒ…è¶£', 'è¿·å¥¸', 'å†…å°„', 'åç²¾', 'å ç²¾', 'è¡¥è‚¾', 'å·æƒ…', 'å¼ºå¥¸', 'æ‰å¥¸', 'è½®å¥¸', 'çŒ®å¦»', 'ç™½ç¿˜', 'å® å¹¸', 'å±çœ¼', 'AVç´ äºº', 'å•æ‹', 'è‚¥è‡€', 
#             'æ·«æ°´', 'å•ªå•ª', 'å¥³ä¼˜', 'å¥³ ä¼˜', 'å¢å¤§å¢ç²—', 'æ½®å¹', 'æ½® å¹', 'åƒå±Œ', 'åƒ å±Œ', 'è£¸ç…§', 'porn']
# stopwords_zh = ['ï¼Œ', 'çš„', 'ã€‚', 'ã€', 'ï¼š', 'å’Œ', 'åœ¨', 'ä¸­', 'æœ', 'å›½', 'äº†', 'æ˜¯', 'â€œ', 'â€', 'ä¸º', '#', ':', 'ï¼ˆ', 'ï¼‰', '35520398', 'ğŸ±@', '_', 'â€¦@', '.', 'o', 'å»',
#                 '(', ')', '-', ',', '|', 'å°±', 'ä»–ä»¬', '@', 'â€¦#', 'è¦', 'æœ‰', 'çœ‹', 'é‡Œ', 'åœ°', 'æ‰€', 'ä¸€', 'ğŸ±#', '/', 'ã€‘', 'å¯¹', 'ï¼', 'å¤ª', 'çœŸ', 'ç€', 'å¾ˆ', 'ã€', 'å…¥',
#                 'å€‘', 'å†™', 'å› ä¸º', 'ä¹°', 'æ¥', 'ä¹Ÿ', 'ã€‚\x00PLAYTHE']
# stopwords_ja = ['ã®', 'ãŒ', 'ã§', 'ã‚’', 'ã¯', '\x00@', 'ãŸ', 'ã«', 'ã¦', 'ã‚Œ', 'ã—', 'ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', 'â€œ', 'â€', '#', ':', 'ï¼ˆ', 'ï¼‰', 'ğŸ±@', '_', 'â€¦@', '.', 'o', 
#                 '(', ')', '-', ',', '|', '@', 'â€¦#', 'ä¸€', 'ğŸ±#', '/', 'ã€‘', 'ã€']
# stopwords_ko = []
# stopwords = {'zh': stopwords_zh, 'ja': stopwords_ja, 'ko': stopwords_ko}
# ä¸éœ€è¦å†™å¤§å†™çš„åœç”¨è¯ï¼Œç¨‹åºé‡Œåˆ¤æ–­æ—¶å°†textéƒ½å˜å°å†™äº†ã€‚
stopwords = ['the', 'to', 'in', 'and', 'including', 'of', 'as', 'has', '&amp;', 'by', 'out', 'is', 'on', 'with', 'a', 'other', 'had', 'will', 'issues', 'issue',
            'several', 'be', 'auâ€¦@gauravcsawant:', 'already', 'citing', 'new', 'over', 'wechat.\x00noâ€¦@abhijitmajumder:', 'full', 'story:', 'but', 'are',
            'about', '-', 'looking', 'i', 'his', 'for', 'this', 'at','it', 'after', 'he', 'same', 'from', 'faces', 'added', "it,'", 'amid', "'we're", 'now', 'do', 'come',
            'comes', 'case', 'says', 'into', 'more', 'also', 'should', 'than', 'used', 'due', 'all', 'between', 'its', 'an', 'how', 'my', 'see', 'you', 'name', 'after',
            'not', "what's", 'what', '\x00\x00', 'that', 'another', 'one', 'why', 'have', 'here', 'can', 'may', 'going', 'want', 'where', 'would', 'or', 'they', 'it,', 
            'same,', 'was', 'use', 'let', 'get', 'we', 'shouldnâ€™t', "shouldn't", 'Itâ€™s', "It's", 'been', 'by', 'visited', 'visit', 'said', 'if', 'much', 'able', 'many',
            'say', 'big', 'which', 'set', 'some', 'make', 'made', 'makes', 'like']

def delurl(text, url):
    for i in range (len(url)):
        text = text.replace(str(url[i]), '')
    return text

def dataprep(text, lang):
    url = re.findall(r'http[a-zA-Z0-9\.\?\/\&\=\:\^\%\$\#\!]*', text)
    text = delurl(text, url)
    if lang == 'zh':
        text = thuseg.cut(text, text=True)
    if lang == 'ja':
        text = mecab_tagger.parse(text)
    text = text.replace("\n", "\0")
    if text[0:2] == 'RT':
        text_delRT = text[3:]
    else:
        text_delRT = text
    return text_delRT

def words_frequency(inputfile, outputfile1, outputfile2, stopwords_):
    # è·å¾—è¯é¢‘
    print ("1ã€å¼€å§‹è®¡ç®—è¯é¢‘...")
    wordlist = inputfile.split()
    counted_words = []
    words_count = {}
    for i in range(len(wordlist)):
        if wordlist[i] not in counted_words:
            counted_words.append(wordlist[i])
            words_count[wordlist[i]] = 1
            for j in range(i+1, len(wordlist)):
                if wordlist[i] == wordlist[j]:
                    words_count[wordlist[i]] += 1
    # åˆå¹¶è¯ä¸å¯¹åº”é¢‘æ•°
    print ("2ã€åˆå¹¶è¯ä¸å¯¹åº”é¢‘æ•°...")
    words = []
    counts = []
    for items in words_count:
        words.append(items)
        counts.append(words_count[items])
    combined = list(zip(words, counts))
    # å†’æ³¡æ³•æ’å¤§å°ï¼Œè·å¾—æœªå»é™¤åœç”¨è¯çš„è¯é¢‘è¡¨
    print ("3ã€å†’æ³¡æ³•æ’å¤§å°...")
    for k in range(len(combined)-1):
        for i in range(len(combined)-1):
            if combined[i][1] < combined[i+1][1]:
                variable = combined[i]
                combined[i] = combined[i+1]
                combined[i+1] = variable
    with open (file_result + "{}.txt".format(outputfile1), "w", encoding='utf-8') as file:
        file.write(str(combined))
    # å»é™¤åœç”¨è¯åçš„è¯é¢‘è¡¨
    print ("4ã€å»é™¤åœç”¨è¯...")
    combined_no_stopwords = []
    for n in range(len(combined)):
        if combined[n][0].lower() in stopwords_:
            continue
        combined_no_stopwords.append(combined[n])
    with open (file_result + "{}.txt".format(outputfile2), "w", encoding='utf-8') as file:
        file.write(str(combined_no_stopwords))

print ("å¼€å§‹æ±‡æ€»è¯­æ®µï¼š")

full_text = {}
for state in relevant_countries:
    for business in relevant_companies:
        pair = state + '_' + business
        full_text[pair] = ''
# print (pairs)
for filename in filenames:
    print (filename)
    countries_companies = filename.split('.')[0]
    [countries, companies] = countries_companies.split('_')
    countries = countries.split('-')
    companies = companies.split('-')
    countries_real = []
    companies_real = []
    for country in countries:
        if country in relevant_countries:
            countries_real.append(country)
    for company in companies:
        if company in relevant_companies:
            companies_real.append(company)
    if countries_real == [] or companies_real == []:
        continue
    filepath = filedir + '/' + filename
    with open (filepath, 'r', encoding='utf-8') as f:
        for tweets in jsonlines.Reader(f):
            text = dataprep(tweets['text'], 'en')
            with open (file_result + "full_text" + ".txt", "a", encoding='utf-8') as file:
                file.write(text + ' ')
            for country in countries_real:
                for company in companies_real:
                    pair = country + '_' + company
                    full_text[pair] += text

with open (file_result + 'full_text.jsonl', "w", encoding='utf-8') as file:
    # file.write(str(text_all))
    file.write(json.dumps(full_text)+'\n')

print ("å¼€å§‹æ±‚è¯é¢‘ï¼š")

for country in relevant_countries:
    for company in relevant_companies:
        pair = country + '_' + company
        print (pair)
        content = ''
        for filename in filenames:
            if country in filename and company in filename :
                # countries_companies = filename.split('.')[0]
                # [countries, companies] = countries_companies.split('_')
                # countries = countries.split('-')
                # companies = companies.split('-')
                content += full_text[pair]
        try:                                      # é¿å…for_draw...æ–‡ä»¶å¤¹å½±å“
            # stopwords_ = stopwords[filename[0:2]]
            words_frequency(content, 'words_frequency_original_' + pair, 'words_frequency_no_stopwords_' + pair, stopwords)
        except:
            assert filename == 'for_draw_word_vector'
            pass

print ("å®Œæˆï¼")
